{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b369209-1d7c-41f8-8449-59624e82f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 43676.6209, Val Loss: 0.0933\n",
      "Epoch 2/20, Loss: 27124.4065, Val Loss: 0.0977\n",
      "Epoch 3/20, Loss: 24854.3658, Val Loss: 0.0822\n",
      "Epoch 4/20, Loss: 22832.9364, Val Loss: 0.0833\n",
      "Epoch 5/20, Loss: 26678.8785, Val Loss: 0.0813\n",
      "Epoch 6/20, Loss: 26477.5563, Val Loss: 0.1089\n",
      "Epoch 7/20, Loss: 20810.0554, Val Loss: 0.0859\n",
      "Epoch 8/20, Loss: 23368.6646, Val Loss: 0.0873\n",
      "Epoch 9/20, Loss: 21424.4339, Val Loss: 0.0530\n",
      "Epoch 10/20, Loss: 23758.5531, Val Loss: 0.0674\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"preProcessed_FeatureClean_AttackTypes.csv\")\n",
    "\n",
    "# Drop non-numeric columns if needed (assuming 'Label' is the target)\n",
    "features = df.drop(columns=['Label'])\n",
    "target = df['Label']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders for parallel processing\n",
    "train_dataset = TensorDataset(X_train)\n",
    "val_dataset = TensorDataset(X_val)\n",
    "num_workers = 64  # Number of workers for DataLoader\n",
    "\n",
    "# Define the auto-encoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'hidden_dim1': [64, 128],\n",
    "    'hidden_dim2': [32, 64],\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initialize results\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Perform grid search\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # Initialize the model with given parameters\n",
    "    model = Autoencoder(input_dim=X.shape[1], hidden_dim1=params['hidden_dim1'], hidden_dim2=params['hidden_dim2'])\n",
    "    optimizer = Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    # Create DataLoader instances with multiple workers\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[0]  # Extract tensor from dataset tuple\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[0]\n",
    "                val_outputs = model(batch)\n",
    "                val_loss += loss_fn(val_outputs, batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_score:\n",
    "        best_score = val_loss\n",
    "        best_params = params\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Validation Loss:\", best_score)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val)\n",
    "    val_loss = loss_fn(val_outputs, X_val).item()\n",
    "print(\"Final Validation Loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b072d-d4c9-40fe-840f-1fa699ef3b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
